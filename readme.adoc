= Parallel and Concurrent Programming - Anthony Sanchez Quiros

Comprehensive academic coursework from a university-level Parallel and Concurrent Programming course. Features 47+ hands-on exercises and assignments covering POSIX pthreads, OpenMP, MPI, and task-based concurrency patterns. Includes implementations of classic synchronization problems (dining philosophers, producer-consumer, readers-writers), distributed computing algorithms, and progressive optimization projects from serial to highly parallel solutions. Demonstrates practical application of mutex, semaphores, condition variables, collective operations, and performance analysis techniques.

== Pthreads

[%autowidth]
|===
|# |Id |Type |Description
|1 |link:pthreads/commented_examples[commented_examples] |Homework | Basic example of pthread_create.
|2 |link:pthreads/recursive_hello[recursive_hello] |Homework | Recursive creation of threads.
|3 |link:pthreads/invert[invert] |Class_example |Serial program that inverts integer numbers given in the standard input.
|4 |link:pthreads/recursive_hello/trace[recursive_hello_trace] |Homework | Memory trace of recursive_hello.
|5 |link:pthreads/hello_w[hello_w] |Homework | Creating and joining threads with for cycle.
|6 |link:pthreads/grandma_lottery[grandma_lottery] |Class_example |Serial program that inverts integer numbers given in the standard input.
|7 |link:pthreads/hello_iw_pri[hello_iw_pri] |Homework | Basic example of thread private data.
|8 |link:pthreads/hello_iw_shr[hello_iw_shr] |Homework | Basic example of thread shared data.
|9 |link:pthreads/hello_order_busywait[hello_order_busywait] |Homework | Basic example of thread busy waiting.
|10 |link:pthreads/team_shot_put[team_shot_put] |Homework | The program takes as input n teams, each consisting of m athletes. Each athlete performs multiple throws, and their best attempt is recorded. The program uses threads to handle the execution efficiently. After all throws are processed, the team with the highest combined best throws is declared the winner.
|11 |link:pthreads/create_thread_team[create_thread_team] |Class_example | Exercise to solve team_shot_put using pthread teams instead of normal pthreads.
|12 |link:pthreads/race_position[race_position] |Homework | This program simulates a race condition among threads, where only one thread can reach position x, but the outcome is non-deterministic.
|13 |link:pthreads/hello_order_semaphor[hello_order_semaphor] |Homework | This program allows only one thread to print a greeting at the same time using a semaphore for the threads.
|14 |link:pthreads/birthday_pinata[birthday_pinata] |Class_example | This program simulates a birthday party scenario where all threads must hit the pinata until it breaks. The thread that breaks the pi√±ata must announce it.
|15 |link:pthreads/hello_order_cond_safe[hello_order_cond_safe] |Homework | A program that allows threads to print in order using conditional safety to avoid busy-waiting.
|16 |link:pthreads/building_tasks[building_tasks] |Class_Example | This exercise involves fixing a concurrent program that simulates task execution based on a dependency graph, ensuring that no worker thread starts before all its prerequisite tasks are completed.
|17 |link:pthreads/prod_cons_bound[prod_cons_bound] | Homework |This exercise focuses on the bounded buffer problem, where a fixed-size buffer is shared between producer and consumer threads that must be synchronized to avoid overflows and underflows.
|18 |link:pthreads/prod_cons_bound_mod[prod_cons_bound_mod] | Homework | This exercise addresses the unbounded buffer problem, where producer and consumer threads interact with a buffer that can grow dynamically, requiring synchronization but without the constraint of a fixed size.
|19 |link:pthreads/prod_cons_unbound_cpp[prod_cons_unbound] | Homework | Producer-consumer problem with an unbounded buffer implemented in C++.
|20 |link:pthreads/readers_writers[readers_writers] | Class example | Classic readers-writers problem implementation using pthreads.
|21 |link:pthreads/readers_writers_rwlock[readers_writers_rwlock] | Class example | Readers-writers problem using read-write locks.
|22 |link:pthreads/delayed_busy_wait[delayed_busy_wait] | Class example | Demonstrates busy waiting with configurable delays.
|23 |link:pthreads/mist_cond_var[mist_cond_var] | Class example | Implementation using condition variables for thread synchronization.
|===


== OpenMP

[%autowidth]
|===
|# |Id |Type |Description
|1 |link:omp/omp_team[omp_team] | Class example | Example of team parallelism using OpenMP.
|2 |link:omp/omp_for[omp_for] | Class example | Example of parallel for loops using OpenMP.
|3 |link:omp/omp_several_for_stages[omp_several_for_stages] | Class example | Example of multiple parallel for stages with OpenMP.
|4 |link:omp/omp_mergesort[omp_mergesort] | Class example | Parallel mergesort implementation using OpenMP.
|5 |link:omp/omp_stats[omp_stats] | Class example | Statistical computations using OpenMP parallelism.
|===

== MPI (Message Passing Interface)

[%autowidth]
|===
|# |Id |Type |Description
|1 |link:mpi/mpi_hello[mpi_hello] | Class example | Basic MPI hello world example.
|2 |link:mpi/mpi_wrapper[mpi_wrapper] | Class example | C++ wrapper class for MPI initialization and management.
|3 |link:mpi/mpi_hybrid_distr_arg[hybrid_distr_arg] | Class example | MPI program that distributes command line arguments across processes.
|4 |link:mpi/hybrid_distr_stdin[hybrid_distr_stdin] | Class example | MPI program that distributes standard input data across processes.
|5 |link:mpi/send_recv_ord_sem[send_recv_ord_sem] | Class example | MPI point-to-point communication with ordered semaphore synchronization.
|6 |link:mpi/send_recv_ord_itm[send_recv_ord_itm] | Class example | MPI point-to-point communication with ordered item synchronization.
|7 |link:mpi/send_recv_urd[send_recv_urd] | Class example | MPI unordered point-to-point communication example.
|8 |link:mpi/mpi_ping_pong[mpi_ping_pong] | Class example | MPI ping-pong communication simulation between two processes.
|9 |link:mpi/mpi_relay_race[mpi_relay_race] | Class example | MPI relay race simulation with process coordination.
|10 |link:mpi/hybrid_distr_bcast[hybrid_distr_bcast] | Class example | MPI broadcast communication with hybrid distribution.
|11 |link:mpi/mpi_lucky_number_reduce[mpi_lucky_number_reduce] | Class example | MPI collective reduction operation to find lucky numbers.
|12 |link:mpi/mpi_lucky_number_who[mpi_lucky_number_who] | Class example | MPI program to determine which process has the lucky number.
|13 |link:mpi/dining_philosophers[dining_philosophers] | Class example | MPI implementation of the dining philosophers problem.
|===

== Task-based Concurrency

[%autowidth]
|===
|# |Id |Type |Description
|1 |link:taskc/network_simul_packet_loss[network_simul_packet_loss] | Class Example | Simulates network packet loss in a concurrent environment.
|2 |link:taskc/network_simul_packet_loss2[network_simul_packet_loss2] | Class Example | Variation of the network packet loss simulation.
|3 |link:taskc/network_simul_producers[network_simul_producers] | Class example | Simulates multiple producers in a network environment.
|4 |link:taskc/network_simul_bounded[network_simul_bounded] | Class example | Simulates a bounded network buffer with concurrent producers and consumers.
|5 |link:taskc/taskc_patterns[taskc_patterns] | Class Example | Examples of common concurrency patterns.
|6 |link:taskc/build_h2o[build_h2o] | Class example | Task-based simulation of water molecule formation (H2O).
|7 |link:taskc/dancing_pairs[dancing_pairs] | Class example | Task-based simulation of dancing pairs synchronization problem.
|===

== Major Assignments

[%autowidth]
|===
|# |Id |Type |Description
|1 |link:homeworks/serial[serial] | Assignment 01 | Serial program that stabilizes the temperatures of an n x m sheet, processing one cell at a time.
|2 |link:homeworks/pthread[pthread] | Assignment 02 | Parallel program that stabilizes temperatures using POSIX threads, processing multiple cells simultaneously.
|3 |link:homeworks/optimized[optimized] | Assignment 03 | Optimized parallel program with more efficient algorithms than the pthread version.
|4 |link:homeworks/omp_mpi[omp_mpi] | Assignment 04 | Implementation using OpenMP and MPI for distributed parallel processing.
|5 |link:homeworks/omp_mpi_V2[omp_mpi_V2] | Assignment 05 | Enhanced version of the OpenMP/MPI implementation with performance improvements.
|===

== Performance Analysis

[%autowidth]
|===
|# |Id |Type |Description
|1 |link:datap/amdahl_compare_ab[amdahl_compare_ab] | Homework | Performance comparison and speedup analysis using Amdahl's Law.
|===


== Glossary

=== Serial Programming

A programming paradigm where task execution is sequential, meaning task1 must be completed before proceeding to task2.

=== Concurrent Programming

A programming paradigm that can be based on imperative or functional programming. Unlike serial programming, concurrent programming can perform multiple tasks at once. For example, while task01 is finishing, task2 can be started.

=== Parallel Programming

Refers to programming that is capable of performing multiple tasks simultaneously. That is, executing task1 and task2 at the same time, at the same instant.

=== Task Concurrency

Refers to when multiple threads access the same data, which can generate race condition problems.

=== Data Parallelism

Refers to when a dataset is divided into fragments and each fragment is processed in parallel on different processing units.

=== Thread of Execution

A thread of execution refers to an executor capable of performing tasks in parallel with other executors (threads).

=== Non-determinism

Non-determinism is when it's unknown which thread will finish its work first. For example, sending 2 threads to perform exactly the same task - sometimes thread 1 will finish first, sometimes thread 2.

=== Private and Shared Memory

Private memory refers to variables, data, and structs that only one thread has access to. Shared memory is that which multiple threads have access to.

=== Busy Waiting

Refers to when threads are waiting for a condition to be met in order to perform their assigned task, but while waiting they consume CPU resources, severely slowing it down. This is the worst programming practice that can be performed.

=== Race Condition

A race condition is when n threads perform procedures based on data that is being read and modified concurrently, generating undesired non-deterministic results.

=== Concurrency Control

A method that avoids busy waiting of threads, making them wait without consuming resources instead of waiting while consuming them. Threads can be said to "sleep".

=== Conditional Safety

A concurrency control method that makes n threads "wait" until a certain condition is met.

=== Mutual Exclusion

A concurrency control tool that prevents more than one thread from accessing data, avoiding possible race conditions.

=== Semaphore

A concurrency control tool that allows n threads to perform an activity at the same time, sending a signal each time the semaphore frees one of the available spaces, allowing other threads to access the activity.

=== Barrier

A concurrency control tool that allows n threads to wait until all have reached the same point, without the need for busy waiting.

=== Condition Variable

A concurrency control tool that allows threads to wait until a specific condition is met, allowing other threads to modify the condition state and notify waiting threads.

=== Read-Write Lock

A concurrency control tool that allows multiple threads to read data simultaneously, but only allows one thread to write to the data at a time, avoiding race conditions.

=== Decomposition

Refers to the separation of work into multiple tasks, facilitating the use of concurrency.

=== Mapping

Refers to the assignment of tasks to threads, allowing each thread to perform a specific task.

=== Speedup

Refers to the improvement in the amount of time required for program completion.

=== Efficiency

The relationship between the execution time of a program considering the number of threads used and the execution time of the program using only one thread.

=== Point-to-Point Process Communication

Communication between process A and process B, without intervention from another process.

=== Collective Process Communication

Communication between multiple processes.

=== Reduction

Refers to the operation of combining results from multiple processes into a single result, for example, summing the results of multiple threads into a single result.
